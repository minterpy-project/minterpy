{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "18832584-2604-4143-b435-d224d8fcc778",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# Polynomial Regression with Minterpy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60effc2d-4ece-438e-ad0f-3d683703d964",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "The polynomials you've encountered so far in these tutorials are interpolating polynomials designed to approximate functions of interest based on values at specific, strategically chosen points known as the unisolvent nodes.\n",
    "This also means that you have the flexibility to evaluate the function at these chosen points within its domain.\n",
    "\n",
    "However, situations may arise where function values are available only at arbitrary, scattered points across the domain.\n",
    "In such cases, you cannot select the evaluation points freely.\n",
    "Despite this limitation, it is still possible to derive a polynomial that best fits the scattered data using the least squares method.\n",
    "\n",
    "In this in-depth tutorial, you'll learn how to perform polynomial regression in Minterpy to construct a polynomial that fits scattered data. This process involves finding the polynomial that minimizes the error between the observed values and the values predicted by the polynomial."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d5175a7-c1c6-4095-97b2-30aa2fc5f8b0",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "As usual, before you begin, you'll need to import the necessary packages to follow along with this guide."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e79dfcd0-a294-4cfe-9e67-7fb88eeb4579",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import minterpy as mp\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89be973a-7635-433b-90a4-7fa2345d43be",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## Motivating problem\n",
    "\n",
    "Consider the following two-dimensional transcendental functions:\n",
    "\n",
    "$$\n",
    "f(\\boldsymbol{x}) = \\frac{1}{\\sqrt{2}} \\left( f_1(x_1) f_1(x_2) + f_2(x_1) f_2(x_2) \\right),\n",
    "$$\n",
    "\n",
    "where\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "f_1(x) & = \\frac{1}{\\pi^{1/4}} e^{-x^2/2} \\\\\n",
    "f_2(x) & = \\frac{2 x}{\\sqrt{\\pi}} f_1(x),\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "and $\\boldsymbol{x} = (x_1, x_2) \\in [-1, 1]^2$.\n",
    "\n",
    "Given a dataset of pairs $\\{ (\\boldsymbol{x}^{(i)}, y^{(i)}) \\}_{i = 1}^N$, your task is to construct a polynomial that approximates the function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0168b1bf-7e8f-419f-ab6f-68d42a16646f",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "First, define the above functions in Python:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f86db01-c589-4155-8b05-576c046ac8d7",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def fun_1(x):\n",
    "    return 1 / np.pi**(1 / 4) * np.exp(-0.5 * x**2)\n",
    "\n",
    "def fun_2(x):\n",
    "    return fun_1(x) * (2 * x) / np.sqrt(np.pi)\n",
    "\n",
    "def fun(xx):\n",
    "    return 1 / np.sqrt(2) * fun_1(xx[:, 0]) * fun_1(xx[:, 1]) + fun_2(xx[:, 0]) * fun_2(xx[:, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e15c738-98a5-4275-a872-1eb6569cacf3",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "Then, generate the dataset randomly, say with $100$ points:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d173cf3e-6f8b-4941-8008-2139920c68f8",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "rng = np.random.default_rng(9271)\n",
    "xx_train = -1 + 2 * rng.random((100, 2))\n",
    "yy_train = fun(xx_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53dc9e61-7a51-4ceb-8bd8-9960fcdd7694",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "The plots of the function along with the given dataset are shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0294c557-31a5-4468-88c5-575e413b1453",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "\n",
    "# --- Create 2D data\n",
    "xx_1d = np.linspace(-1.0, 1.0, 500)[:, np.newaxis]\n",
    "mesh_2d = np.meshgrid(xx_1d, xx_1d)\n",
    "xx_2d = np.array(mesh_2d).T.reshape(-1, 2)\n",
    "yy_2d = fun(xx_2d)\n",
    "\n",
    "# --- Create two-dimensional plots\n",
    "fig = plt.figure(figsize=(10, 5))\n",
    "\n",
    "# Surface\n",
    "axs_1 = plt.subplot(121, projection='3d')\n",
    "axs_1.plot_surface(\n",
    "    mesh_2d[0],\n",
    "    mesh_2d[1],\n",
    "    yy_2d.reshape(500, 500).T,\n",
    "    linewidth=0,\n",
    "    cmap=\"plasma\",\n",
    "    antialiased=False,\n",
    "    alpha=0.4,\n",
    ")\n",
    "axs_1.scatter(\n",
    "    xx_train[:, 0],\n",
    "    xx_train[:, 1],\n",
    "    yy_train,\n",
    "    color=\"k\",\n",
    "    marker=\"x\",\n",
    ")\n",
    "axs_1.set_xlabel(\"$x_1$\", fontsize=14)\n",
    "axs_1.set_ylabel(\"$x_2$\", fontsize=14)\n",
    "axs_1.set_title(\"Surface plot ($f$)\", fontsize=14)\n",
    "axs_1.tick_params(axis='both', which='major', labelsize=12)\n",
    "\n",
    "# Contour\n",
    "axs_2 = plt.subplot(122)\n",
    "cf = axs_2.contourf(\n",
    "    mesh_2d[0], mesh_2d[1], yy_2d.reshape(500, 500).T, cmap=\"plasma\"\n",
    ")\n",
    "axs_2.scatter(xx_train[:, 0], xx_train[:, 1], color=\"k\", marker=\"x\")\n",
    "axs_2.set_xlabel(\"$x_1$\", fontsize=14)\n",
    "axs_2.set_ylabel(\"$x_2$\", fontsize=14)\n",
    "axs_2.set_title(\"Contour plot ($f$)\", fontsize=14)\n",
    "divider = make_axes_locatable(axs_2)\n",
    "cax = divider.append_axes('right', size='5%', pad=0.05)\n",
    "fig.colorbar(cf, cax=cax, orientation='vertical')\n",
    "axs_2.axis('scaled')\n",
    "axs_2.tick_params(axis='both', which='major', labelsize=12)\n",
    "\n",
    "fig.tight_layout(pad=6.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81923c45-50aa-4d5a-be01-13f0f344863e",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## Polynomial regression\n",
    "\n",
    "In polynomial regression, we construct a linear system of equations as follows:\n",
    "\n",
    "$$\n",
    "\\boldsymbol{R} \\, \\boldsymbol{c} = \\boldsymbol{y},\n",
    "$$\n",
    "\n",
    "where $\\boldsymbol{R}$, $\\boldsymbol{c}$, $\\boldsymbol{y}$ are the regression matrix, the vector of polynomial coefficients, and the vector of output data, respectively.\n",
    "\n",
    "Each column of the regression matrix $\\boldsymbol{R}$ is obtained by evaluating each of the monomials of the chosen polynomial basis at the each input data.\n",
    "\n",
    "The goal of polynomial regression is to estimate $\\boldsymbol{c}$ given $\\{ (\\boldsymbol{x}^{(i)}, y^{(i)}) \\}_{i = 1}^N$ such that chosen polynomial becomes fully determined."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b654b358-0159-4581-b9d5-d5002557c116",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "### Define a polynomial model\n",
    "\n",
    "Following the steps shown in the previous examples, defining a polynomial starts with defining the underlying multi-index set.\n",
    "\n",
    "For instance, you may define a multi-index set of polynomial degree $8$ and $l_p$-degree $2.0$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "603eaf0b-1b4f-4686-9e4e-c0a0a017c675",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "mi = mp.MultiIndexSet.from_degree(2, 8, 2.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4955e34-75c9-49d2-9ad1-8ebc9d1ff7e7",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "Make sure that the size of the multi-index set is at least smaller than the available data points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b1bf99a-e689-4d1e-a206-799643f8dcc9",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "len(mi)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "514251d1-307f-4fe3-a5b3-dabe8c09e7da",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "### Create an ordinary regression polynomial"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a0f759e-1f93-40b4-b544-b8204de95913",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "The {py:class}`OrdinaryRegression <.extras.regression.ordinary_regression.OrdinaryRegression>` class is used as an interface between polynomials that you've seen before and regression polynomial.\n",
    "\n",
    "To create an instance of {py:class}`OrdinaryRegression <.extras.regression.ordinary_regression.OrdinaryRegression>`, pass the multi-index set to the default constructor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d509374-46e1-4c25-abac-7ffb2e7bc4d3",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "reg_poly = mp.OrdinaryRegression(mi)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe856f10-9c87-405c-a64a-5bbfa3f284c9",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "By default, if not specified, an underlying interpolating grid is created using Leja-ordered Chebyshev-Lobatto points.\n",
    "Additionally, the underlying polynomial is, by default, represented in the Lagrange basis.\n",
    "In other words, ordinary least squares will subsequently be used to estimate the coefficients of the polynomial in the Lagrange basis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d252360-4dea-4509-ad1c-e7449d6ae43e",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "### Fit the regression polynomial\n",
    "\n",
    "You have so far constructed a regression polynomial by defining the structure of the underling polynomial. For now, however, its coefficients remain unknown."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f977b3b2-9db1-43b1-9656-65efd0be74c7",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "reg_poly.coeffs is None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8693dfa7-f6dc-460d-be81-27721116428c",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "The polynomial coefficients are obtained by solving the following least-squares problem (ordinary least-squares):\n",
    "\n",
    "$$\n",
    "\\hat{\\boldsymbol{c}} = \\underset{\\boldsymbol{c}}{\\mathrm{arg min}} \\lVert \\boldsymbol{R} \\boldsymbol{c} - \\boldsymbol{y} \\rVert_2^2\n",
    "$$\n",
    "\n",
    "where: $\\lVert \\cdot \\rVert_2^2$ denotes the square of the Euclidian norm.\n",
    "\n",
    "\n",
    "You can use the method {py:meth}`fit() <.extras.regression.ordinary_regression.OrdinaryRegression.fit>` to solve the above problem on the given regression polynomial instance.\n",
    "The method requires a pair of inputs and the corresponding output are required. As these are already constructed above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76174cad-e150-41dc-8deb-77322a2782cd",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "reg_poly.fit(xx_train, yy_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c04cd06e-c1c4-4cb5-83be-63205c9a6079",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "Note that the call to `fit()` alters the instance in-place; once the call is concluded, the coefficients are available:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e75466de-b2ea-4ed8-be95-0df98466c569",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "reg_poly.coeffs[:10]  # the first 10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dbc5ff1-0ec2-4bc9-bb43-660694860493",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "### Evaluate the regression polynomial\n",
    "\n",
    "Once fitted, you can evaluate the instance at a set of query points.\n",
    "\n",
    "For instance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c271b0f-47f6-46f6-b7af-923dcc9b81e5",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "reg_poly(\n",
    "    np.array([\n",
    "        [-1., 1.],\n",
    "        [-0.5, 0.0],\n",
    "        [0.0, 0.5],\n",
    "    ])\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "165fe13e-683b-4c8f-afb4-abb7ebaf12db",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "### Verify the fitted polynomial"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5722e6e-4981-4aca-b7e5-3987f7c6ff59",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "You can also check several measures that indicate the quality of the fitted polynomial by calling the {py:meth}`show() <.extras.regression.ordinary_regression.OrdinaryRegression.show>`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b394e75-8da2-4900-b871-ebf2f9617b38",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "reg_poly.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6736b70b-b0de-4c5a-9aa6-ea0bea2d026b",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "The three measures of fitting error are:\n",
    "\n",
    "- Infinity norm (`L-inf`)\n",
    "- Euclidian norm (`L2`)\n",
    "- Leave-one-out cross validation error\n",
    "\n",
    "For `L2` and `LOO CV` the relative errors are the absolute errors normalized by the variance of the output in the dataset. For `L-inf` the relative error is the absolute error normalized by the standard deviation of the output in the dataset.\n",
    "\n",
    "These fitting error measures are solely based on the training dataset. You should view them critically; small values do not necessarily means highly accurate regression polynomial as they tend to underestimate the generalization error (i.e., error based on the unseen data).\n",
    "That being said, the LOO-CV error is usually a more robust estimate of the generalization error than the other two.\n",
    "\n",
    "To estimate the generalization error in L2 sense:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6758515-af29-4f3f-8d70-c284ba90c4a3",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "xx_test = -1 + 2 * rng.random((10000, 2))\n",
    "yy_test = fun(xx_test)\n",
    "print(np.mean((yy_test - reg_poly(xx_test))**2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b06f3cb-1561-41ba-9fdf-3f043f485fa4",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "The plots below show the surface, contour, and the error plot for the regression polynomial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10ff2e99-e484-4ad1-9d35-649d2f6c186a",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "\n",
    "# --- Create 2D data\n",
    "xx_1d = np.linspace(-1.0, 1.0, 500)[:, np.newaxis]\n",
    "mesh_2d = np.meshgrid(xx_1d, xx_1d)\n",
    "xx_2d = np.array(mesh_2d).T.reshape(-1, 2)\n",
    "yy_fun_2d = fun(xx_2d)\n",
    "yy_reg_2d = reg_poly(xx_2d)\n",
    "yy_err_2d = np.abs(yy_fun_2d - yy_reg_2d)\n",
    "\n",
    "# --- Create two-dimensional plots\n",
    "fig = plt.figure(figsize=(15, 5))\n",
    "\n",
    "# Surface\n",
    "axs_1 = plt.subplot(131, projection='3d')\n",
    "axs_1.plot_surface(\n",
    "    mesh_2d[0],\n",
    "    mesh_2d[1],\n",
    "    yy_reg_2d.reshape(500, 500).T,\n",
    "    linewidth=0,\n",
    "    cmap=\"plasma\",\n",
    "    antialiased=False,\n",
    "    alpha=0.4,\n",
    ")\n",
    "axs_1.scatter(\n",
    "    xx_train[:, 0],\n",
    "    xx_train[:, 1],\n",
    "    yy_train,\n",
    "    color=\"k\",\n",
    "    marker=\"x\",\n",
    ")\n",
    "axs_1.set_xlabel(\"$x_1$\", fontsize=14)\n",
    "axs_1.set_ylabel(\"$x_2$\", fontsize=14)\n",
    "axs_1.set_title(\"Surface plot ($Q$)\", fontsize=14)\n",
    "axs_1.tick_params(axis='both', which='major', labelsize=12)\n",
    "\n",
    "# Contour\n",
    "axs_2 = plt.subplot(132)\n",
    "cf = axs_2.contourf(\n",
    "    mesh_2d[0], mesh_2d[1], yy_reg_2d.reshape(500, 500).T, cmap=\"plasma\"\n",
    ")\n",
    "axs_2.scatter(xx_train[:, 0], xx_train[:, 1], color=\"k\", marker=\"x\")\n",
    "axs_2.set_xlabel(\"$x_1$\", fontsize=14)\n",
    "axs_2.set_ylabel(\"$x_2$\", fontsize=14)\n",
    "axs_2.set_title(\"Contour plot ($Q$)\", fontsize=14)\n",
    "divider = make_axes_locatable(axs_2)\n",
    "cax = divider.append_axes('right', size='5%', pad=0.05)\n",
    "fig.colorbar(cf, cax=cax, orientation='vertical')\n",
    "axs_2.axis('scaled')\n",
    "axs_2.tick_params(axis='both', which='major', labelsize=12)\n",
    "\n",
    "# Contour Eror\n",
    "axs_3 = plt.subplot(133)\n",
    "cf = axs_3.contourf(\n",
    "    mesh_2d[0], mesh_2d[1], yy_err_2d.reshape(500, 500).T, cmap=\"plasma\"\n",
    ")\n",
    "axs_3.scatter(xx_train[:, 0], xx_train[:, 1], color=\"k\", marker=\"x\")\n",
    "axs_3.set_xlabel(\"$x_1$\", fontsize=14)\n",
    "axs_3.set_ylabel(\"$x_2$\", fontsize=14)\n",
    "axs_3.set_title(\"Error ($| f - Q |$)\", fontsize=14)\n",
    "divider = make_axes_locatable(axs_3)\n",
    "cax = divider.append_axes('right', size='5%', pad=0.05)\n",
    "fig.colorbar(cf, cax=cax, orientation='vertical')\n",
    "axs_3.axis('scaled')\n",
    "axs_3.tick_params(axis='both', which='major', labelsize=12)\n",
    "\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fce35b9-442f-4066-9ab7-63e7bcbab389",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "The plots show that the regression polynomial captures the overall behavior of the original function well. This indicates that the chosen polynomial structure and the available data are sufficient. Furthermore, the error plot shows that the differences between the regression polynomial and the original function are small across the function domain."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d98e8ff5-a681-41a8-ab68-32ff946139e1",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "### Closing remark\n",
    "\n",
    "Once fitted, a regression polynomial is fully determined. In particular, the `eval_poly` property of the instance is a Minterpy polynomial you've already familiar. You can carry out arithmetic as well as calculus operations on it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98087335-a980-4afc-99c4-ab9830b8ce3b",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "reg_poly.eval_poly"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "238eb241-2c75-4528-8dc0-bffc4b97478d",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "By default, the evaluation polynomial is set to be a polynomial in the Newton basis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fb479b9-6a53-4b0a-a01c-89c8806d470e",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## Summary\n",
    "\n",
    "In this tutorial, you've learned how to perform polynomial regression in Minterpy.\n",
    "This is particularly useful when you have a dataset (pairs of inputs and outputs) but you may not have access to the underlying function.\n",
    "\n",
    "Polynomial regression in Minterpy is facilitated through the {py:class}`OrdinaryRegression <.extras.regression.ordinary_regression.OrdinaryRegression>` class.\n",
    "The steps to construct a regression polynomial are as follows:\n",
    "\n",
    "- Determine the structure of the underlying polynomial by specifying the multi-index set.\n",
    "- Create an instance of {py:class}`OrdinaryRegression <.extras.regression.ordinary_regression.OrdinaryRegression>` based on the defined polynomial structure.\n",
    "- Fit the instance with the given dataset using the {py:meth}`fit() <.extras.regression.ordinary_regression.OrdinaryRegression.fit>` method.\n",
    "- Verify the regression polynomial using the {py:meth}`show() <.extras.regression.ordinary_regression.OrdinaryRegression.show>` method.\n",
    "\n",
    "Once fitted, the resulting instance is callable, allowing you to evaluate it at any set of query points by passing these points directly to the instance.\n",
    "\n",
    "Finally, you can also access the underlying fitted polynomial from the {py:meth}`eval_poly <.extras.regression.ordinary_regression.OrdinaryRegression.eval_poly>` property of the instance.\n",
    "This polynomial is a Minterpy polynomial, so you can apply a variety of arithmetic and calculus operations on it, as covered in the previous tutorials.\n",
    "\n",
    "---\n",
    "\n",
    "Congratulations on completing the in-depth tutorials of Minterpy!\n",
    "\n",
    "You should now be familiar with the main features of Minterpy.\n",
    "We hope you find Minterpy valuable for your work!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
